{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSE9I4DnBrs0"
      },
      "outputs": [],
      "source": [
        "#create similar micrograd library for neural networks from scratch using numpy library and data types"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary items\n",
        "#neurons:\n",
        "#have input x and weights w\n",
        "#\n",
        "#\n",
        "#layers\n",
        "#perceptron"
      ],
      "metadata": {
        "id": "lRauqkVdB2o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#represent a single neuron in a neural network layer\n",
        "#attributes: weights corresponding to each input value, single bias per neuron, number of input to neuron\n",
        "#methods: convert input to output - forward, compute gradient - backward\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, n_input, nonlin=True):\n",
        "        self.w = np.random.randn(n_input, 1) * np.sqrt(2 / n_input)\n",
        "        self.b = np.zeros((1, 1))\n",
        "        self.nonlin = nonlin\n",
        "        self.params = self.w + self.b\n",
        "        self.activ = lambda x: None\n",
        "        self.activ_deriv = lambda x: None\n",
        "        self.last_input = None  # should be a np array of inputs\n",
        "        self.last_output = None  # should be a np array of outputs\n",
        "\n",
        "        # Fixing the logic here: nonlin=True should be ReLU\n",
        "        if self.nonlin:\n",
        "            self.activ = lambda x: np.where(x > 0, x, 0.01 * x)  # Leaky ReLU activation\n",
        "            self.activ_deriv = lambda x: np.where(x > 0, 1, 0.01)\n",
        "        else:\n",
        "            self.activ = lambda x: x  # Linear activation\n",
        "            self.activ_deriv = lambda x: 1  # Linear derivative\n",
        "\n",
        "    def __repr__(self):\n",
        "        out = f\"Input to neuron: {len(self.w)} Parameters: {self.params}\"\n",
        "        return out\n",
        "\n",
        "    # Takes input vector, multiplies with weights, and adds bias to return scalar output\n",
        "    def forward(self, input):\n",
        "        self.last_input = input\n",
        "        out = np.dot(input, self.w) + self.b\n",
        "        self.last_output = self.activ(out)\n",
        "        return self.last_output\n",
        "\n",
        "    # Backpropagates the loss and updates parameters\n",
        "    def backward(self, post_activation_loss, lr):\n",
        "        #given loss of output of neuron after activation, compute gradient for input of current neuron and update parameters going of(going into) neuron\n",
        "        #1. since each neuron goes through activation, need to compute derivative of activation from last output of current neuron\n",
        "        #2. use gradient from post activation neuron ouput * derivative of activation = gradient of pre activation output\n",
        "        d_activation = self.activ_deriv(self.last_output)\n",
        "        d_out = post_activation_loss * d_activation #gradient wrt loss of x1w1 + x2w2 + b = value of neuron output before activation\n",
        "\n",
        "        # Compute Gradients for weights and bias:\n",
        "        # Now that we have the gradient with respect to the pre activation output of current neuron,\n",
        "        # Compute specific gradients with respect to weight and bias using this gradient\n",
        "        d_w = np.dot(self.last_input.T, d_out)  # weight gradient will be X*pre_activation_output_grad\n",
        "        d_b = np.sum(d_out, axis=0, keepdims=True) # bias gradient is the same as gradient of pre activation output, sum rows for all examples in batch (bias shared across all examples equally)\n",
        "\n",
        "        # Update weights and bias\n",
        "        self.w -= lr * d_w\n",
        "        self.b -= lr * d_b\n",
        "\n",
        "        # Return gradient of input to current neuron (X), to previous layer, as gradient of OUTPUT of previous layer with updates\n",
        "        # dL/dout (x=input to layer, output from previous layer) * w = dL/dout * dout/dx == dL/dx\n",
        "        return np.dot(d_out, self.w.T)\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, n_input_per_neuron, n_neurons, nonlin=True):\n",
        "        self.neurons = [Neuron(n_input_per_neuron, nonlin) for i in range(n_neurons)]\n",
        "        self.n_input_per_neuron = n_input_per_neuron\n",
        "        self.n_neurons = n_neurons\n",
        "\n",
        "    def __repr__(self):\n",
        "        output = f\"Number of input to neurons: {self.n_input_per_neuron} Number of neurons: {self.n_neurons}\"\n",
        "        return output\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.last_input = input\n",
        "        self.last_output = np.hstack([neuron.forward(input) for neuron in self.neurons])#concatenated output of each neuron in list in order\n",
        "        return self.last_output\n",
        "\n",
        "    def backward(self, loss_prev_list, lr):\n",
        "        #initialize grad accumulator with zeros of same shape of input from single neuron - each neuron has same input in same layer\n",
        "        grad_accum = np.zeros_like(self.neurons[0].last_input)\n",
        "\n",
        "        #for each neuron in order, perform backpropagation using loss from subsequent neurons as loss wrt. output of current neuron\n",
        "        #1. From output layer we send the loss gradient - m(examples)x n output neurons column vectors ex: 3x2\n",
        "        #2. For each neuron in output layer, compute the gradient of input from previous layer (X) and update weights\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            grad = neuron.backward(loss_prev_list[:, [i]], lr)#all examples gradients for specific neuron\n",
        "            if i == 0:\n",
        "                grad_accum = grad\n",
        "            else:\n",
        "                grad_accum += grad\n",
        "        return grad_accum\n",
        "\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    loss = np.mean((y_pred - y_true) ** 2)\n",
        "    return loss\n",
        "\n",
        "def mse_derivative(y_pred, y_true):\n",
        "    grad = 2 * (y_pred - y_true) / y_true.shape[0]\n",
        "    return grad\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layer_sizes, nonlin=True):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(Layer(layer_sizes[i], layer_sizes[i + 1], nonlin))\n",
        "\n",
        "        self.prediction = 0\n",
        "        self.loss = 0\n",
        "        self.grad = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.layers[0].forward(input)\n",
        "        for layer in self.layers[1:]:\n",
        "            out = layer.forward(out)\n",
        "        self.prediction = out\n",
        "        return self.prediction\n",
        "\n",
        "    def backward(self, grad, lr):#should not call forward\n",
        "        for layer in reversed(self.layers):\n",
        "          grad = layer.backward(grad, lr)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate=.001, batch_size=16):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                y_pred = self.forward(X_batch)\n",
        "                loss = mse_loss(y_pred, y_batch)\n",
        "                total_loss += loss * len(X_batch)\n",
        "\n",
        "                output_gradient = mse_derivative(y_pred, y_batch)\n",
        "                self.backward(output_gradient, learning_rate)\n",
        "\n",
        "            avg_loss = total_loss / len(X)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "id": "sarsyc50Ee9Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([[1,2],[1,1],[0,1]])\n",
        "y_pred = np.array([[.67,1.1],[.44,1.5],[.77,.99]])"
      ],
      "metadata": {
        "id": "w6rLhU_CyuHE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_grad = mse_derivative(y_pred, y_true)#shape(batch_size,num_output_neurons)"
      ],
      "metadata": {
        "id": "GexBEdPay-2U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = Layer(3,2)"
      ],
      "metadata": {
        "id": "jvtye9n_zduB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.forward(np.array([[1,2,3],[2,3,4],[5,6,7]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKYF5pvoz-AD",
        "outputId": "8193ecb1-fd01-4ab3-8bb0-cc4232c38a55"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01062592,  0.86169118],\n",
              "       [-0.0293614 ,  1.81149586],\n",
              "       [-0.08556783,  4.66090991]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-QJ1GN0y-Yt",
        "outputId": "539d916f-2236-4320-fcc9-fe9ca082534b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_grad[:, [0]], output_grad[:, [1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbb6HF7p7f2p",
        "outputId": "d86f8561-3716-4a95-92b7-a388b6f3305a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.22      ],\n",
              "        [-0.37333333],\n",
              "        [ 0.51333333]]),\n",
              " array([[-0.6       ],\n",
              "        [ 0.33333333],\n",
              "        [-0.00666667]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.backward(output_grad, lr=.01).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPydaKuwzkxc",
        "outputId": "b2ff3af0-3d5b-482d-f520-86427904e394"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "mnist_train = pd.read_csv('/content/sample_data/mnist_train_small.csv')"
      ],
      "metadata": {
        "id": "5sS_gjb-AO3A"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "f8R7969ZBF3W",
        "outputId": "cb6d0613-84bf-484d-e6b4-8b369ca322c1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       6  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.581  0.582  0.583  \\\n",
              "0      5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "1      7  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "2      9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "3      5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "4      2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "...   .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...   \n",
              "19994  0  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "19995  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "19996  2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "19997  9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "19998  5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              "\n",
              "       0.584  0.585  0.586  0.587  0.588  0.589  0.590  \n",
              "0          0      0      0      0      0      0      0  \n",
              "1          0      0      0      0      0      0      0  \n",
              "2          0      0      0      0      0      0      0  \n",
              "3          0      0      0      0      0      0      0  \n",
              "4          0      0      0      0      0      0      0  \n",
              "...      ...    ...    ...    ...    ...    ...    ...  \n",
              "19994      0      0      0      0      0      0      0  \n",
              "19995      0      0      0      0      0      0      0  \n",
              "19996      0      0      0      0      0      0      0  \n",
              "19997      0      0      0      0      0      0      0  \n",
              "19998      0      0      0      0      0      0      0  \n",
              "\n",
              "[19999 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbd219ba-f177-47a3-aecb-b5fb4aa6f4b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "      <th>...</th>\n",
              "      <th>0.581</th>\n",
              "      <th>0.582</th>\n",
              "      <th>0.583</th>\n",
              "      <th>0.584</th>\n",
              "      <th>0.585</th>\n",
              "      <th>0.586</th>\n",
              "      <th>0.587</th>\n",
              "      <th>0.588</th>\n",
              "      <th>0.589</th>\n",
              "      <th>0.590</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19999 rows × 785 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbd219ba-f177-47a3-aecb-b5fb4aa6f4b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dbd219ba-f177-47a3-aecb-b5fb4aa6f4b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dbd219ba-f177-47a3-aecb-b5fb4aa6f4b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-88c987c8-8f63-444f-8844-c98474f40ace\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-88c987c8-8f63-444f-8844-c98474f40ace')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-88c987c8-8f63-444f-8844-c98474f40ace button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_76590b51-21d0-4b3e-9945-96ce7627186f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('mnist_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_76590b51-21d0-4b3e-9945-96ce7627186f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('mnist_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mnist_train"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = mnist_train.iloc[:,0]\n",
        "#y_train = y_train.reshape(len(y_train),1)#get 2d array of y labels\n",
        "\n",
        "x_train = mnist_train.iloc[:,1:]"
      ],
      "metadata": {
        "id": "ankGTUisBCRc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "    # scale all pixel values from 0-255 to 0-1\n",
        "x_train = np.array(x_train.astype(np.float32) / 255.0)\n",
        "#X_test = x_train.astype(np.float32) / 255.0\n",
        "\n",
        "# one-hot encode the labels\n",
        "    # represent categorical data as binary vectors\n",
        "y_train = np.eye(10)[y_train]\n",
        "#y_test_onehot = np.eye(10)[y_test]\n"
      ],
      "metadata": {
        "id": "zlTiDGYZWmFn"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape, x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAuoo3ZtQYxd",
        "outputId": "157731a2-aae1-4509-eaf3-d0407edce1c1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((19999, 10), (19999, 784))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Define network architecture: 2 inputs, 3 neurons in hidden layer, 1 output\n",
        "  nn = Network(layer_sizes = [784, 128, 64, 10])\n",
        "  nn.train(x_train, y_train, epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8qQA34lSrv3",
        "outputId": "ce50aacd-1622-421d-b147-382b30831119"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.08174047578319589\n",
            "Epoch 2/10, Loss: 0.057068117946955745\n",
            "Epoch 3/10, Loss: 0.049930327996763994\n",
            "Epoch 4/10, Loss: 0.04486909032324081\n",
            "Epoch 5/10, Loss: 0.04140568948682911\n",
            "Epoch 6/10, Loss: 0.03952990039655105\n",
            "Epoch 7/10, Loss: 0.03826307127808743\n",
            "Epoch 8/10, Loss: 0.037300843468398626\n",
            "Epoch 9/10, Loss: 0.03645898716607613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Network(layer_sizes = [784, 128, 64, 10])"
      ],
      "metadata": {
        "id": "4BJNSyYSz0Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Define network architecture: 2 inputs, 3 neurons in hidden layer, 1 output\n",
        "    nn = Network([2, 3, 1])\n",
        "\n",
        "    # Example input and output, call with 2d arrays\n",
        "    X = np.array([[1, 2], [2, 3], [3, 4]])  # Batch of 3 samples with 2 features each\n",
        "    y = np.array([[1], [0], [1]])  # Binary outputs for this example\n",
        "\n",
        "    # Forward pass\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "      predictions = nn.forward(X)\n",
        "      print(f\"Predictions: {predictions}\")\n",
        "      # Backward pass\n",
        "      nn.backward(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDKpU9kZJE6t",
        "outputId": "466959ea-bc2d-4cc2-9c75-da89afc5ef35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[2.08464602]\n",
            " [3.50175676]\n",
            " [4.91886751]]\n",
            "MSE Loss: 9.598760001711051\n",
            "Predictions: [[0.64473241]\n",
            " [1.21768521]\n",
            " [1.79063802]]\n",
            "MSE Loss: 0.7446936063668862\n",
            "Predictions: [[0.36138424]\n",
            " [0.76807176]\n",
            " [1.17475929]]\n",
            "MSE Loss: 0.34276837726783554\n",
            "Predictions: [[0.2813222 ]\n",
            " [0.63956348]\n",
            " [0.99780477]]\n",
            "MSE Loss: 0.3085146839906245\n",
            "Predictions: [[0.2584644 ]\n",
            " [0.60131691]\n",
            " [0.94416942]]\n",
            "MSE Loss: 0.3048580429000102\n",
            "Predictions: [[0.25264908]\n",
            " [0.58995929]\n",
            " [0.92726951]]\n",
            "MSE Loss: 0.3039583634737935\n",
            "Predictions: [[0.25197893]\n",
            " [0.58672605]\n",
            " [0.92147318]]\n",
            "MSE Loss: 0.3033164832466461\n",
            "Predictions: [[0.25286535]\n",
            " [0.58595808]\n",
            " [0.9190508 ]]\n",
            "MSE Loss: 0.3027032732793373\n",
            "Predictions: [[0.25421985]\n",
            " [0.58593886]\n",
            " [0.91765787]]\n",
            "MSE Loss: 0.30209753446312737\n",
            "Predictions: [[0.25571154]\n",
            " [0.58614659]\n",
            " [0.91658164]]\n",
            "MSE Loss: 0.3014972524648111\n",
            "Predictions: [[0.25723974]\n",
            " [0.58642242]\n",
            " [0.91560511]]\n",
            "MSE Loss: 0.30090218835739385\n",
            "Predictions: [[0.25877381]\n",
            " [0.58671798]\n",
            " [0.91466215]]\n",
            "MSE Loss: 0.3003122681426785\n",
            "Predictions: [[0.26030445]\n",
            " [0.58701851]\n",
            " [0.91373256]]\n",
            "MSE Loss: 0.2997274337248151\n",
            "Predictions: [[0.26182888]\n",
            " [0.58731951]\n",
            " [0.91281015]]\n",
            "MSE Loss: 0.29914762917214194\n",
            "Predictions: [[0.26334627]\n",
            " [0.58761964]\n",
            " [0.91189301]]\n",
            "MSE Loss: 0.29857279946366894\n",
            "Predictions: [[0.26485643]\n",
            " [0.58791848]\n",
            " [0.91098052]]\n",
            "MSE Loss: 0.2980028903710066\n",
            "Predictions: [[0.26635933]\n",
            " [0.58821591]\n",
            " [0.91007248]]\n",
            "MSE Loss: 0.2974378484382391\n",
            "Predictions: [[0.267855 ]\n",
            " [0.5885119]\n",
            " [0.9091688]]\n",
            "MSE Loss: 0.2968776209687272\n",
            "Predictions: [[0.2693435 ]\n",
            " [0.58880646]\n",
            " [0.90826942]]\n",
            "MSE Loss: 0.29632215601206524\n",
            "Predictions: [[0.27082488]\n",
            " [0.58909959]\n",
            " [0.90737431]]\n",
            "MSE Loss: 0.29577140235107513\n",
            "Predictions: [[0.27229919]\n",
            " [0.58939131]\n",
            " [0.90648342]]\n",
            "MSE Loss: 0.295225309488986\n",
            "Predictions: [[0.2737665 ]\n",
            " [0.58968162]\n",
            " [0.90559674]]\n",
            "MSE Loss: 0.2946838276368427\n",
            "Predictions: [[0.27522686]\n",
            " [0.58997053]\n",
            " [0.90471421]]\n",
            "MSE Loss: 0.29414690770117163\n",
            "Predictions: [[0.27668031]\n",
            " [0.59025806]\n",
            " [0.90383582]]\n",
            "MSE Loss: 0.2936145012718937\n",
            "Predictions: [[0.27812693]\n",
            " [0.59054422]\n",
            " [0.90296152]]\n",
            "MSE Loss: 0.2930865606104825\n",
            "Predictions: [[0.27956675]\n",
            " [0.59082902]\n",
            " [0.90209129]]\n",
            "MSE Loss: 0.29256303863836836\n",
            "Predictions: [[0.28099983]\n",
            " [0.59111246]\n",
            " [0.90122509]]\n",
            "MSE Loss: 0.2920438889255744\n",
            "Predictions: [[0.28242623]\n",
            " [0.59139457]\n",
            " [0.9003629 ]]\n",
            "MSE Loss: 0.2915290656795841\n",
            "Predictions: [[0.283846  ]\n",
            " [0.59167534]\n",
            " [0.89950468]]\n",
            "MSE Loss: 0.2910185237344345\n",
            "Predictions: [[0.28525918]\n",
            " [0.59195479]\n",
            " [0.8986504 ]]\n",
            "MSE Loss: 0.2905122185400299\n",
            "Predictions: [[0.28666583]\n",
            " [0.59223293]\n",
            " [0.89780004]]\n",
            "MSE Loss: 0.2900101061516686\n",
            "Predictions: [[0.288066  ]\n",
            " [0.59250978]\n",
            " [0.89695356]]\n",
            "MSE Loss: 0.2895121432197811\n",
            "Predictions: [[0.28945973]\n",
            " [0.59278533]\n",
            " [0.89611093]]\n",
            "MSE Loss: 0.28901828697987225\n",
            "Predictions: [[0.29084708]\n",
            " [0.5930596 ]\n",
            " [0.89527212]]\n",
            "MSE Loss: 0.28852849524266483\n",
            "Predictions: [[0.29222809]\n",
            " [0.5933326 ]\n",
            " [0.89443711]]\n",
            "MSE Loss: 0.2880427263844379\n",
            "Predictions: [[0.29360281]\n",
            " [0.59360434]\n",
            " [0.89360587]]\n",
            "MSE Loss: 0.287560939337559\n",
            "Predictions: [[0.29497128]\n",
            " [0.59387483]\n",
            " [0.89277837]]\n",
            "MSE Loss: 0.28708309358119827\n",
            "Predictions: [[0.29633356]\n",
            " [0.59414407]\n",
            " [0.89195458]]\n",
            "MSE Loss: 0.28660914913223134\n",
            "Predictions: [[0.29768969]\n",
            " [0.59441208]\n",
            " [0.89113447]]\n",
            "MSE Loss: 0.28613906653631627\n",
            "Predictions: [[0.29903971]\n",
            " [0.59467887]\n",
            " [0.89031803]]\n",
            "MSE Loss: 0.28567280685914814\n",
            "Predictions: [[0.30038367]\n",
            " [0.59494444]\n",
            " [0.88950521]]\n",
            "MSE Loss: 0.2852103316778831\n",
            "Predictions: [[0.30172161]\n",
            " [0.59520881]\n",
            " [0.888696  ]]\n",
            "MSE Loss: 0.28475160307272857\n",
            "Predictions: [[0.30305358]\n",
            " [0.59547198]\n",
            " [0.88789037]]\n",
            "MSE Loss: 0.2842965836186984\n",
            "Predictions: [[0.30437962]\n",
            " [0.59573395]\n",
            " [0.88708829]]\n",
            "MSE Loss: 0.28384523637752634\n",
            "Predictions: [[0.30569977]\n",
            " [0.59599475]\n",
            " [0.88628973]]\n",
            "MSE Loss: 0.28339752488973624\n",
            "Predictions: [[0.30701408]\n",
            " [0.59625438]\n",
            " [0.88549468]]\n",
            "MSE Loss: 0.2829534131668647\n",
            "Predictions: [[0.30832259]\n",
            " [0.59651285]\n",
            " [0.88470311]]\n",
            "MSE Loss: 0.28251286568383177\n",
            "Predictions: [[0.30962533]\n",
            " [0.59677016]\n",
            " [0.88391499]]\n",
            "MSE Loss: 0.2820758473714601\n",
            "Predictions: [[0.31092235]\n",
            " [0.59702632]\n",
            " [0.8831303 ]]\n",
            "MSE Loss: 0.28164232360913444\n",
            "Predictions: [[0.31221368]\n",
            " [0.59728135]\n",
            " [0.88234901]]\n",
            "MSE Loss: 0.28121226021760204\n",
            "Predictions: [[0.31349938]\n",
            " [0.59753524]\n",
            " [0.88157111]]\n",
            "MSE Loss: 0.2807856234519098\n",
            "Predictions: [[0.31477948]\n",
            " [0.59778802]\n",
            " [0.88079656]]\n",
            "MSE Loss: 0.2803623799944738\n",
            "Predictions: [[0.31605401]\n",
            " [0.59803967]\n",
            " [0.88002534]]\n",
            "MSE Loss: 0.2799424969482807\n",
            "Predictions: [[0.31732302]\n",
            " [0.59829023]\n",
            " [0.87925744]]\n",
            "MSE Loss: 0.27952594183021734\n",
            "Predictions: [[0.31858654]\n",
            " [0.59853968]\n",
            " [0.87849282]]\n",
            "MSE Loss: 0.279112682564524\n",
            "Predictions: [[0.31984461]\n",
            " [0.59878804]\n",
            " [0.87773147]]\n",
            "MSE Loss: 0.27870268747637156\n",
            "Predictions: [[0.32109728]\n",
            " [0.59903532]\n",
            " [0.87697336]]\n",
            "MSE Loss: 0.27829592528555863\n",
            "Predictions: [[0.32234456]\n",
            " [0.59928152]\n",
            " [0.87621848]]\n",
            "MSE Loss: 0.27789236510032395\n",
            "Predictions: [[0.32358652]\n",
            " [0.59952665]\n",
            " [0.87546679]]\n",
            "MSE Loss: 0.27749197641127604\n",
            "Predictions: [[0.32482317]\n",
            " [0.59977072]\n",
            " [0.87471828]]\n",
            "MSE Loss: 0.2770947290854338\n",
            "Predictions: [[0.32605455]\n",
            " [0.60001374]\n",
            " [0.87397293]]\n",
            "MSE Loss: 0.27670059336037806\n",
            "Predictions: [[0.3272807 ]\n",
            " [0.60025571]\n",
            " [0.87323071]]\n",
            "MSE Loss: 0.2763095398385091\n",
            "Predictions: [[0.32850166]\n",
            " [0.60049664]\n",
            " [0.87249161]]\n",
            "MSE Loss: 0.2759215394814119\n",
            "Predictions: [[0.32971746]\n",
            " [0.60073653]\n",
            " [0.8717556 ]]\n",
            "MSE Loss: 0.27553656360432205\n",
            "Predictions: [[0.33092813]\n",
            " [0.6009754 ]\n",
            " [0.87102267]]\n",
            "MSE Loss: 0.27515458387069575\n",
            "Predictions: [[0.33213371]\n",
            " [0.60121325]\n",
            " [0.87029279]]\n",
            "MSE Loss: 0.2747755722868744\n",
            "Predictions: [[0.33333423]\n",
            " [0.60145009]\n",
            " [0.86956595]]\n",
            "MSE Loss: 0.27439950119685097\n",
            "Predictions: [[0.33452972]\n",
            " [0.60168592]\n",
            " [0.86884212]]\n",
            "MSE Loss: 0.27402634327712827\n",
            "Predictions: [[0.33572022]\n",
            " [0.60192075]\n",
            " [0.86812128]]\n",
            "MSE Loss: 0.2736560715316691\n",
            "Predictions: [[0.33690575]\n",
            " [0.60215459]\n",
            " [0.86740342]]\n",
            "MSE Loss: 0.27328865928694074\n",
            "Predictions: [[0.33808636]\n",
            " [0.60238744]\n",
            " [0.86668851]]\n",
            "MSE Loss: 0.27292408018704595\n",
            "Predictions: [[0.33926208]\n",
            " [0.60261931]\n",
            " [0.86597655]]\n",
            "MSE Loss: 0.2725623081889418\n",
            "Predictions: [[0.34043292]\n",
            " [0.60285021]\n",
            " [0.8652675 ]]\n",
            "MSE Loss: 0.2722033175577441\n",
            "Predictions: [[0.34159893]\n",
            " [0.60308014]\n",
            " [0.86456135]]\n",
            "MSE Loss: 0.27184708286211584\n",
            "Predictions: [[0.34276014]\n",
            " [0.60330911]\n",
            " [0.86385808]]\n",
            "MSE Loss: 0.27149357896973747\n",
            "Predictions: [[0.34391658]\n",
            " [0.60353713]\n",
            " [0.86315767]]\n",
            "MSE Loss: 0.27114278104285666\n",
            "Predictions: [[0.34506827]\n",
            " [0.60376419]\n",
            " [0.86246012]]\n",
            "MSE Loss: 0.27079466453391815\n",
            "Predictions: [[0.34621524]\n",
            " [0.60399031]\n",
            " [0.86176539]]\n",
            "MSE Loss: 0.27044920518126986\n",
            "Predictions: [[0.34735754]\n",
            " [0.6042155 ]\n",
            " [0.86107346]]\n",
            "MSE Loss: 0.27010637900494455\n",
            "Predictions: [[0.34849518]\n",
            " [0.60443976]\n",
            " [0.86038434]]\n",
            "MSE Loss: 0.26976616230251627\n",
            "Predictions: [[0.34962819]\n",
            " [0.60466309]\n",
            " [0.85969798]]\n",
            "MSE Loss: 0.26942853164502817\n",
            "Predictions: [[0.35075661]\n",
            " [0.6048855 ]\n",
            " [0.85901439]]\n",
            "MSE Loss: 0.2690934638729924\n",
            "Predictions: [[0.35188045]\n",
            " [0.605107  ]\n",
            " [0.85833354]]\n",
            "MSE Loss: 0.2687609360924596\n",
            "Predictions: [[0.35299976]\n",
            " [0.60532759]\n",
            " [0.85765541]]\n",
            "MSE Loss: 0.268430925671156\n",
            "Predictions: [[0.35411456]\n",
            " [0.60554727]\n",
            " [0.85697999]]\n",
            "MSE Loss: 0.26810341023468814\n",
            "Predictions: [[0.35522486]\n",
            " [0.60576606]\n",
            " [0.85630726]]\n",
            "MSE Loss: 0.26777836766281277\n",
            "Predictions: [[0.35633071]\n",
            " [0.60598396]\n",
            " [0.85563721]]\n",
            "MSE Loss: 0.2674557760857716\n",
            "Predictions: [[0.35743213]\n",
            " [0.60620098]\n",
            " [0.85496982]]\n",
            "MSE Loss: 0.2671356138806885\n",
            "Predictions: [[0.35852915]\n",
            " [0.60641711]\n",
            " [0.85430507]]\n",
            "MSE Loss: 0.2668178596680288\n",
            "Predictions: [[0.35962178]\n",
            " [0.60663237]\n",
            " [0.85364295]]\n",
            "MSE Loss: 0.26650249230811895\n",
            "Predictions: [[0.36071007]\n",
            " [0.60684676]\n",
            " [0.85298345]]\n",
            "MSE Loss: 0.2661894908977262\n",
            "Predictions: [[0.36179402]\n",
            " [0.60706028]\n",
            " [0.85232654]]\n",
            "MSE Loss: 0.2658788347666958\n",
            "Predictions: [[0.36287368]\n",
            " [0.60727295]\n",
            " [0.85167221]]\n",
            "MSE Loss: 0.26557050347464584\n",
            "Predictions: [[0.36394906]\n",
            " [0.60748476]\n",
            " [0.85102045]]\n",
            "MSE Loss: 0.2652644768077172\n",
            "Predictions: [[0.36502019]\n",
            " [0.60769572]\n",
            " [0.85037124]]\n",
            "MSE Loss: 0.26496073477538035\n",
            "Predictions: [[0.3660871 ]\n",
            " [0.60790583]\n",
            " [0.84972457]]\n",
            "MSE Loss: 0.26465925760729325\n",
            "Predictions: [[0.36714981]\n",
            " [0.60811511]\n",
            " [0.84908042]]\n",
            "MSE Loss: 0.26436002575021533\n",
            "Predictions: [[0.36820834]\n",
            " [0.60832356]\n",
            " [0.84843878]]\n",
            "MSE Loss: 0.2640630198649698\n",
            "Predictions: [[0.36926272]\n",
            " [0.60853117]\n",
            " [0.84779963]]\n",
            "MSE Loss: 0.2637682208234591\n",
            "Predictions: [[0.37031297]\n",
            " [0.60873796]\n",
            " [0.84716296]]\n",
            "MSE Loss: 0.2634756097057305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBPxgUjhhEpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = Network([3,2,1])"
      ],
      "metadata": {
        "id": "EJ1UF7X-2GLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = Layer(3,2)"
      ],
      "metadata": {
        "id": "iZrEzH9hw4iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.forward(np.array([3,2,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o30HFCGwyM56",
        "outputId": "ffd134d2-5646-4d64-da34-285f19099f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.54419492]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.backward(np.array([10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "E_OI0AOP7FHG",
        "outputId": "accec304-7109-4ebc-c920-1b7964f2c0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss: [[55.58902943]]\n",
            "[[-14.91161016]]\n",
            "[-14.91161016]\n",
            "[[2.66345899 2.16573133]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b281327d79a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-cd9a9fd7f067>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, y_true)\u001b[0m\n\u001b[1;32m    108\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cd9a9fd7f067>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss_prev_list, lr)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#print(loss_prev_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#return np.sum([neuron.backward(loss_prev_list[:, [i]], lr) for i, neuron in enumerate(self.neurons)], axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_prev_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cd9a9fd7f067>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#print(loss_prev_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#return np.sum([neuron.backward(loss_prev_list[:, [i]], lr) for i, neuron in enumerate(self.neurons)], axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_prev_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cd9a9fd7f067>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss_prev, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0md_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#derivative of bias with respect to loss = derivative of out wrt loss * derivative of out wrt bias (=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "import os\n",
        "class Neuron:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.weights = np.random.randn(num_inputs, 1) * 0.01\n",
        "        self.bias = np.zeros((1, 1))\n",
        "        self.last_input = None\n",
        "        self.last_output = None\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        self.last_input = activations\n",
        "        z = np.dot(activations, self.weights) + self.bias\n",
        "        self.last_output = self.relu(z)\n",
        "        return self.last_output\n",
        "\n",
        "    def backward(self, dC_da, learning_rate):\n",
        "        da_dz = self.relu_derivative(self.last_output)\n",
        "        dC_dz = dC_da * da_dz\n",
        "        dC_dw = np.dot(self.last_input.T, dC_dz)\n",
        "        dC_db = np.sum(dC_dz, axis=0, keepdims=True)\n",
        "\n",
        "        self.weights -= learning_rate * dC_dw\n",
        "        self.bias -= learning_rate * dC_db\n",
        "\n",
        "        return np.dot(dC_dz, self.weights.T)\n",
        "\n",
        "\n",
        "    # output_gradient:\n",
        "        # A positive gradient means we need to decrease that output\n",
        "        # A negative gradient means we need to increase that output\n",
        "\n",
        "    # learning_rate: how big of a step is taken while updating weights and biases\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, num_neurons, num_inputs_per_neuron):\n",
        "        self.neurons = [Neuron(num_inputs_per_neuron) for _ in range(num_neurons)]\n",
        "\n",
        "    def forward(self, activations):\n",
        "        return np.hstack([neuron.forward(activations) for neuron in self.neurons])\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.sum([neuron.backward(output_gradient[:, [i]], learning_rate) for i, neuron in enumerate(self.neurons)], axis=0)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(Layer(layer_sizes[i+1], layer_sizes[i]))\n",
        "\n",
        "    def forward(self, activations):\n",
        "        for layer in self.layers:\n",
        "            activations = layer.forward(activations)\n",
        "        return activations\n",
        "\n",
        "    def mse_loss(self, y, activations):\n",
        "        return np.mean((activations-y)**2)\n",
        "\n",
        "    def derivative_mse_loss(self, y, activations):\n",
        "        return 2*(activations-y) / y.shape[0]\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                outputs = self.forward(X_batch)\n",
        "                loss = self.mse_loss(y_batch, outputs)\n",
        "                total_loss += loss * len(X_batch)\n",
        "\n",
        "                output_gradient = self.derivative_mse_loss(y_batch, outputs)\n",
        "                for layer in reversed(self.layers):\n",
        "                    output_gradient = layer.backward(output_gradient, learning_rate)\n",
        "\n",
        "            avg_loss = total_loss / len(X)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)"
      ],
      "metadata": {
        "id": "V0DcO4GSxxiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = NeuralNetwork([3,2,1])"
      ],
      "metadata": {
        "id": "aGSep5e8ycZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.forward([3,2,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQXjFdq2yfDK",
        "outputId": "ac876717-c05a-4e02-fcd3-e42b19965fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4riGsqkyls6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}